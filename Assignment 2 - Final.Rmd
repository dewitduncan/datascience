---
title: "Assignment 2"
author: "Wenrui Jiang (451362) and Duncan de Wit (489824)"
date: "14.04.2020 09:00 AM"
output: pdf_document
---
\newpage


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include = FALSE}
library(openxlsx)
library(psych)
library(stargazer)
library(plyr)
library(ggplot2)
library(nnet)
library(e1071)
library(data.table)
library(randomForest)
library(MASS)
library(rpart)
```
## Question 1

Load and inspect the data. Pay particular attention to the number of categories in the categorical variables *category_list*, *country_code* and *region*. Including categorical variables with too many categories can be problematic when estimating models such as a logit or a linear regression model, although less so when estimating tree-based models such as a decision tree or random forest. Explain why. (2 points)

```{r setup, include = FALSE}
# Load the data
setwd("C:/Users/S/Documents/RSM/BIM/BDBA/Week2/Assignment")
dtCrunch <-read.xlsx("crunchbase2015.xlsx")

# Inspect the data
colSums(is.na(dtCrunch))
dtCrunch <- dtCrunch[complete.cases(dtCrunch),]
summary(dtCrunch)
str(dtCrunch)
head(dtCrunch)
tail(dtCrunch)

# Number of categories in categorical variables
categories_cl <- unique(dtCrunch$category_list)
numberOfCategories_cl <- length(categories_cl)
categories_cl

categories_cc <- unique(dtCrunch$country_code)
numberOfCategories_cc <- length(categories_cc)
numberOfCategories_cc

categories_re <- unique(dtCrunch$region)
numberOfCategories_re <- length(categories_re)
numberOfCategories_re
```
When inspecting the data, missing values are spotted in *name*,*category_list*, and *region*. The number of observations is decreased from 39,179 to 38,433 after removing all the missing values. Since dtCrunch is a relatively large dataset, deleting missing values has little impact.

The variable *category_list* has 691 categories, the variable *country_code* has 121 categories and the variable *region* has 972 categories.

Having too many categories is a problem when estimating models such as a logit or a linear regression model since these models assign a coefficient to the categorical variable. Therefore, it is difficult to assign a coefficient to a categorical variable of which it is unclear what the order of categories is. Furthermore, the coefficient becomes very small if there are too many categories and this makes it difficult to interpret the model.
This is less problematic when estimating tree-based models, since these models can specify which exact categories belong to a specific interior node or leave node. Furthermore, these models can choose to exclude the categorical variables that have a small influence on the classification, but this is not possible in regular logit or linear regression models.


## Question 2

Write a function that takes a single factor variable and a parameter *n* as inputs, computes the number of cases per category in the factor variable, and only keeps those *n* categories with the most number of cases and pools all other categories into a category "other". Apply this function to the variables *category_list*, *country_code* and *region*, keeping 10 categories for each variable and pooling all other categories in each variable into a category "other". (2 points)

```{r setup, include = FALSE}
function1<-function(Data,col_name,n){
  DT<-data.table(Data)
  counts<-DT[, .N, by = col_name]
  counts<-counts[order(counts$N,decreasing=TRUE),]
  topN<-head(counts,n)
  nameN<-topN[1:n,1]
  DT[[col_name]][DT[[col_name]] %in% nameN[[col_name]]==FALSE]='other'
  return(DT)
}
dtCrunch=function1(Data=dtCrunch,"category_list",n=10)
dtCrunch=function1(Data=dtCrunch,"country_code",n=10)
dtCrunch=function1(Data=dtCrunch,"region",n=10)

# These have to be placed after Q2, otherwise the function doesn't work
dtCrunch$category_list<-factor(dtCrunch$category_list)
dtCrunch$status<-factor(dtCrunch$status)
dtCrunch$country_code<-factor(dtCrunch$country_code)
dtCrunch$region<-factor(dtCrunch$region)
colTypes<-sapply(dtCrunch, class)
```


## Question 3

Compute the average of the variable *funding_total_usd* per category for the variables *category_list*, *country_code* and *region*. Based on these averages, what are the three characteristics of the startup that is likely to be the most successful in terms of receiving funding? (1 point)

```{r setup, include = FALSE}
# Calculate average funding per category
table_cl <- ddply(dtCrunch,"category_list", summarise, 
      avgFunding = mean(funding_total_usd))
table_cc <- ddply(dtCrunch,"country_code", summarise, 
      avgFunding = mean(funding_total_usd))
table_re <- ddply(dtCrunch,"region", summarise, 
      avgFunding = mean(funding_total_usd))

# Return row with the highest average funding
table_cl[which.max(table_cl$avgFunding),]
table_cc[which.max(table_cc$avgFunding),]
table_re[which.max(table_re$avgFunding),]
```

The three characteristics of the startup that is likely to be the most successful in receiving funding based on the highest average funding are start-ups in Biotechnology (based on *category_list*), from the country China (based on *country_code* CHN) and from the region SF Bay Area (based on *region*). 
However, it is not possible to be from the region Nanjing and the country Grenada. Since the average funding of Grenada is calculated by one value (St. George's University) you could also say that start-ups in infrastructure from Nanjing, China are most potentially most successful in receiving funding. China is the 4th country based on average funding.


## Question 4

Estimate a linear regression model to predict the total amount of funding a company received (variable *funding_total_usd*) using the variables *country_code*, *company_age*, *years_btw_founding_funding*, *status* and *funding_rounds* as attributes. Also run a stepwise regression using backward search. Output both the full model and the model identified as the best model by the stepwise regression and compare them. (1 point)

```{r setup, include = FALSE}
# Remove  the  company ID  column, which is  redundant
dtCrunch$X  <- NULL

# Define  model  specifications
mdlQ4 <- funding_total_usd ~ country_code + company_age + years_btw_founding_funding + status + funding_rounds

# Estimate  models  and  store  results
rsltQ4 <- lm(mdlQ4, data = dtCrunch)

# Make  predictions (within  sample)
predQ4 <- predict(rsltQ4, dtCrunch)

# Perform  the  stepwise  regression using backward search
rsltStep <- lm(mdlQ4, data = dtCrunch)
step <- stepAIC(rsltStep, direction = "backward") 

# Output full model and the best model according to the stepwise regression
step$anova 
```

In the full model (inital model) the variables *country_code*, *company_age*, *years_btw_founding_funding*, *status* and *funding_rounds* are included and in the model identified as the best model by the stepwise regression (final model) only the variables *company_age*, *status* and *funding_rounds* are included.
This means that the variables *country_code* and *years_btw_founding_funding* are excluded by the backward stepwise regression since these have the smallest partial correlation with the target variable *funding_total_usd* while not lowering the predictive power of the model too much.
In short, the stepwise regression shows that the country and the years between the founding of the company and the first time it attracts funding has a relatively small partial correlation with the amount of funding collected by the company in U.S. Dollars.


## Question 5

Randomly create 5 folds containing an equal number of observations. Using these folds for cross-validation, build and compute the average accuracy of a classification tree, a logistic regression model, a SVM, and a random forest predicting whether a company attracted at least U.S.$10,000,000 of funding. Use the variables *category_list*, *country_code*, *region*, *funding_rounds*, *company_age* and *years_btw_founding_funding* as attributes in your model. Briefly explain which model performed the best. (2 points)

```{r setup, include = FALSE}
# Convert numeric into binomial
dtCrunch$funding_total_usd <- factor(as.numeric(dtCrunch$funding_total_usd>10000000),levels = c(0, 1))

# NewData is saved for Q6
NewData <- head(dtCrunch,10000)

# Scaling  the  numeric (num and int) variables
colTypes <- sapply(dtCrunch, class)
colNumeric <- which(colTypes  == "numeric" | colTypes  == "integer")

# Scale  the  quantitative  data  columns 
dtCrunch[, colNumeric] <- scale(dtCrunch[, colNumeric])

# Randomly create 5 folds
dtCrunch <- dtCrunch[sample(1:nrow(dtCrunch)),]
nFolds <- 5
myFolds <- cut(seq(1, nrow(dtCrunch)), 
               breaks = nFolds, 
               labels=FALSE)
table(myFolds)

# Initialize empty vectors to collect results
accTree<-rep(NA, nFolds)
accLogit<-rep(NA, nFolds)
accSVM<-rep(NA, nFolds)
accRF<-rep(NA, nFolds)

# Define the model 
mdlQ5 <- funding_total_usd ~ category_list + country_code + region + funding_rounds + company_age + years_btw_founding_funding

for (i in 1:nFolds) {
  cat("Analysis of fold", i, "\n")
  
  # Define training and test set
  testObsQ5  <- which(myFolds == i, arr.ind = TRUE)
  dsTestQ5 <- dtCrunch[testObsQ5, ]
  dsTrainQ5  <- dtCrunch[-testObsQ5, ]
  
  # Train the models on the training sets
  rsltTree <- rpart(mdlQ5, data = dsTrainQ5, method = "class", 
                    parms = list(split = "information"))
  rsltLogit <- glm(mdlQ5, data = dsTrainQ5, family = "binomial")
  rsltSVM <- svm(mdlQ5, data = dsTrainQ5, type = "C-classification",
                 kernel = "radial")
  m <- round(sqrt((length(all.vars(mdlQ5)) - 1)))
  rsltRF <- randomForest(mdlQ5, data = dsTrainQ5, ntree = 100, mtry = m, 
                       importance = TRUE)
  
  # Predict values for the test sets
  classTree <- predict(rsltTree, dsTestQ5, type = "class")
  predLogit <- predict(rsltLogit, dsTestQ5, type = "response")
  classLogit <- factor(as.numeric(predLogit > 0.5),levels = c(0, 1))
  classSVM <- predict(rsltSVM, dsTestQ5, type = "response")
  classRF <- predict(rsltRF, dsTestQ5, type = "class")
  
  # Measure accuracy and store the results
  accTree[i] <- mean(classTree == dsTestQ5$funding_total_usd)
  accLogit[i] <- mean(classLogit == dsTestQ5$funding_total_usd)
  accSVM[i] <- mean(classSVM == dsTestQ5$funding_total_usd)
  accRF[i] <- mean(classRF == dsTestQ5$funding_total_usd)
}

# Combine the accuracies obtained with the classifiers in a single matrix
accRslt <- cbind(accTree, accLogit, accSVM, accRF)
  
# Summarise the accuracies per technique 
describe(accRslt)

```

These four models have similar performance, among which Decision Tree has an average accuracy of 0.81, and the other three with average accuracy of 0.82. Decision Tree and Logistic Regression has round to zero standard deviation, while Support Vector Machine and Random Forest has 0.01 standard deviation. No model performs significantly better than others, yet Logisitic Regression might be the best model since it has the highest accuracy and lowest standard deviation.


## Question 6

In order to estimate a neural network, examine using cross-validation how many neurons the hidden layer of the neural network should include. To do so, write a function that takes the data, model, and a parameter *n* as inputs, and computes the model performance in terms of accuracy on a randomly split training and test set using a neural network with *n* layers. Using this function in a loop, compute the performance using neural networks with sizes of the hidden layers of *n* = 1, 2, 3, ..., 20. 
Create a graph that looks similar to the graphs on page 79 and page 84 in the session slides. Use the same variables as attributes/predictors as in the previous question. In order to save on computing time, use only the first 10,000 observations of the dataset in this exercise. Which size of the hidden layer would you pick? (2 points)

```{r setup, include = FALSE}
##NewData was selected before making crunchbase in random order
##now also convert Newdata in random order
NewData<-NewData[sample(1:nrow(NewData)),]

##initaialize matrix
accNN<-matrix(NA,nrow=20,ncol=3)
colnames(accNN)<-c("Number of Neurons","Test Accuracy","Train Accuracy")
for (i in 1:20){
  function2<-function(Model,Data,n){
    pctTrain <- 0.7
    numTrain <- round(pctTrain*nrow(Data))
    obsTrain<-sample(1:nrow(Data), numTrain)
    Train<-Data[obsTrain,]
    Test<-Data[-obsTrain,]
    rsltNN<-nnet(Model,data=Train,size = n)
    predTrain<-predict(rsltNN,Train,type="class")
    predTrain<-factor(predTrain,levels=c(0,1))
    predTest<-predict(rsltNN,Test,type="class")
    predTest<-factor(predTest,levels=c(0,1))
    testAcc<-mean(predTest==Test$funding_total_usd)
    trainAcc<-mean(predTrain==Train$funding_total_usd)
    Accuracies<-cbind(n,testAcc,trainAcc)
    return(Accuracies)
  }
accNN[c(i),]<-function2(mdlB,NewData,i)
}
accNN
accNN.df<-as.data.frame(accNN)

# Plots
plot(accNN.df$`Test Accuracy`, type="l",ylim=c(0.7,0.9),xlab="Number of Neurons",ylab="Accuracy")
lines(accNN.df$`Train Accuracy`, col="red")

```

Based on the table accNN and the plot in which black line is Test Accuracy, red line is Train Accuracy, it can be conclude that the optimal number of neurons in the hidder layer is six. When there is six neurons, Test Accuracy reaches its highest value at 0.825. Although Train Accuracy is not highest when n=6, it should not be considered since the higher train accuracy, the more overfitting NNet is.

